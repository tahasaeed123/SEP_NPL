{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb 13 18:43:11 2023\n",
    "\n",
    "@author: pathouli\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(str_in):\n",
    "    import re\n",
    "    tmp = re.sub(\"[^A-Za-z']+\", \" \",str_in).lower().strip().replace(\"  \", \" \")\n",
    "    return tmp\n",
    "\n",
    "def file_clean(path_in):\n",
    "    f = open(path_in, encoding=\"UTF-8\")\n",
    "    tmp = f.read()\n",
    "    f.close()\n",
    "    tmp = clean_text(tmp)\n",
    "    return tmp\n",
    "\n",
    "def read_files(path_in):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    file_list = pd.DataFrame()\n",
    "    for root, dirs, files in os.walk(path_in, topdown=False):\n",
    "        for name in files:\n",
    "            try:\n",
    "                t_path = root + \"/\" + name\n",
    "                file_p = file_clean(t_path)\n",
    "                t_p = root.split(\"/\")[-1:][0]\n",
    "                if len(file_p) > 0:\n",
    "                    file_list = file_list.append(\n",
    "                        {\"body\": file_p, \"label\": t_p\n",
    "                         }, ignore_index=True)\n",
    "            except:\n",
    "                print (t_path)\n",
    "                pass\n",
    "    return file_list\n",
    "\n",
    "def wrd_dictionary(df_in, col_name_in):\n",
    "    import collections\n",
    "    my_dictionaty_t = dict()\n",
    "    for topic_t in df_in.label.unique():\n",
    "        tmp = df_in[df_in.label == topic_t]\n",
    "        tmp = tmp[col_name_in].str.cat(sep=\" \")\n",
    "        wrd_freq = collections.Counter(tmp.split())\n",
    "        my_dictionaty_t[topic_t] = wrd_freq\n",
    "    return my_dictionaty_t\n",
    "\n",
    "def rem_sw(var_in):\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = stopwords.words(\"english\")\n",
    "    tmp = var_in.split()\n",
    "    # tmp_ar = list()\n",
    "    # for word_t in tmp:\n",
    "    #     if word_t not in sw:\n",
    "    #         tmp_ar.append(word_t)\n",
    "    tmp_ar = [word_t for word_t in tmp if word_t not in sw]\n",
    "    tmp_o = ' '.join(tmp_ar)\n",
    "    return tmp_o\n",
    "\n",
    "def write_pickle(obj_in, path_in, name_in):\n",
    "    import pickle\n",
    "    pickle.dump(obj_in, open(path_in + name_in + \".pk\", 'wb'))\n",
    "    \n",
    "def read_pickle(path_in, name_in):\n",
    "    import pickle\n",
    "    the_data_t = pickle.load(open(path_in + name_in + \".pk\", 'rb'))\n",
    "    return the_data_t\n",
    "\n",
    "def wrd_cnt(txt_in):\n",
    "    tmp = len(set(txt_in.split()))\n",
    "    return tmp\n",
    "\n",
    "def stem_fun(txt_in):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stem_tmp = PorterStemmer()\n",
    "    tmp = [stem_tmp.stem(word) for word in txt_in.split()]\n",
    "    tmp = ' '.join(tmp)\n",
    "    # tmp = list()\n",
    "    # for word in txt_in.split():\n",
    "    #     tmp.append(stem_tmp.stem(word))\n",
    "    return tmp\n",
    "\n",
    "def vec_fun(df_in, m_in, n_in, name_in, out_p_in):\n",
    "    #turn into a function called vec_fun and give user ability to set arbitrary ngrams\n",
    "    #and an arbitrary name for the saved pk object\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    import pandas as pd\n",
    "    if name_in == \"vec\":\n",
    "        xform = CountVectorizer(ngram_range=(m_in, n_in))\n",
    "    else:\n",
    "        xform = TfidfVectorizer(ngram_range=(m_in, n_in))\n",
    "    xform_data = pd.DataFrame(xform.fit_transform(df_in).toarray()) #memory hog\n",
    "    xform_data.columns = xform.get_feature_names()\n",
    "    write_pickle(xform, out_p_in, name_in)\n",
    "    return xform_data\n",
    "\n",
    "def chi_fun(df_in, label_in, k_in, path_out, name_in):\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    import pandas as pd\n",
    "    feat_sel = SelectKBest(score_func=chi2, k=k_in)\n",
    "    dim_data = pd.DataFrame(feat_sel.fit_transform(\n",
    "        df_in, label_in))\n",
    "    feat_index = feat_sel.get_support(indices=True)\n",
    "    feature_names = df_in.columns[feat_index]\n",
    "    dim_data.columns = feature_names\n",
    "    write_pickle(feat_sel, path_out, name_in)\n",
    "    return dim_data\n",
    "\n",
    "def jaccard_dist(corp_1, corp_2):\n",
    "    #intersection/union\n",
    "    intersection = set(corp_1.split()).intersection(set(corp_2.split()))\n",
    "    union = set(corp_1.split()).union(set(corp_2.split()))\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def cos_sim_fun(df_a, df_b, label_in):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    cos_matrix = pd.DataFrame(cosine_similarity(\n",
    "        df_a, df_b))\n",
    "    cos_matrix.index = label_in\n",
    "    cos_matrix.columns = label_in\n",
    "    np.array(cos_matrix)\n",
    "    print (np.average(np.array(cos_matrix)))\n",
    "    return cos_matrix\n",
    "\n",
    "def extract_embeddings_pre(df_in, out_path_i, name_in):\n",
    "    #https://code.google.com/archive/p/word2vec/\n",
    "    #https://pypi.org/project/gensim/\n",
    "    #pip install gensim\n",
    "    import pandas as pd\n",
    "    from nltk.data import find\n",
    "    from gensim.models import KeyedVectors\n",
    "    import pickle\n",
    "    def get_score(var):\n",
    "        import numpy as np\n",
    "        tmp_arr = list()\n",
    "        for word in var:\n",
    "            try:\n",
    "                tmp_arr.append(list(my_model_t.get_vector(word)))\n",
    "            except:\n",
    "                pass\n",
    "        tmp_arr\n",
    "        return np.mean(np.array(tmp_arr), axis=0)\n",
    "    word2vec_sample = str(find(name_in))\n",
    "    my_model_t = KeyedVectors.load_word2vec_format(\n",
    "        word2vec_sample, binary=False)\n",
    "    # word_dict = my_model.key_to_index\n",
    "    tmp_out = df_in.str.split().apply(get_score)\n",
    "    tmp_data = tmp_out.apply(pd.Series).fillna(0)\n",
    "    pickle.dump(my_model_t, open(out_path_i + \"embeddings.pkl\", \"wb\"))\n",
    "    pickle.dump(tmp_data, open(out_path_i + \"embeddings_df.pkl\", \"wb\" ))\n",
    "    return tmp_data, my_model_t\n",
    "\n",
    "def domain_train(df_in, path_in, name_in):\n",
    "    #domain specific\n",
    "    import pandas as pd\n",
    "    import gensim\n",
    "    def get_score(var):\n",
    "        import numpy as np\n",
    "        tmp_arr = list()\n",
    "        for word in var:\n",
    "            try:\n",
    "                tmp_arr.append(list(model.wv.get_vector(word)))\n",
    "            except:\n",
    "                pass\n",
    "        tmp_arr\n",
    "        return np.mean(np.array(tmp_arr), axis=0)\n",
    "    model = gensim.models.Word2Vec(df_in.str.split())\n",
    "    model.save(path_in + 'body.embedding')\n",
    "    #call up the model\n",
    "    #load_model = gensim.models.Word2Vec.load('body.embedding')\n",
    "    model.wv.similarity('fish','river')\n",
    "    tmp_data = pd.DataFrame(df_in.str.split().apply(get_score))\n",
    "    return tmp_data, model\n",
    "\n",
    "def pca_fun(df_in, exp_var_in, out_path_i, name_in):\n",
    "    from sklearn.decomposition import PCA\n",
    "    import pandas as pd\n",
    "    my_pca = PCA(n_components=exp_var_in)\n",
    "    pca_vec = pd.DataFrame(my_pca.fit_transform(df_in))\n",
    "    exp_var = sum(my_pca.explained_variance_ratio_)\n",
    "    print (exp_var)\n",
    "    write_pickle(my_pca, out_path_i, name_in)\n",
    "    return pca_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
